{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch1.0_.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcylmhlcy/pytorch1.0_MostUse/blob/master/pytorch1_0_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hhBqyqLfY8H5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "ddb01ba9-ac21-4483-993c-f35389f32ff8"
      },
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 25 12:15:31 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    31W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5S0v_fR9bsSN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import shutil\n",
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "toXI2Ka7b82j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "75b641fc-de5b-4efe-b27b-1df7877c4aeb"
      },
      "cell_type": "code",
      "source": [
        "print(torch.__version__)               # PyTorch version\n",
        "print(torch.version.cuda)              # Corresponding CUDA version\n",
        "print(torch.backends.cudnn.version())  # Corresponding cuDNN version\n",
        "print(torch.cuda.get_device_name(0))   # GPU type"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.1.post2\n",
            "10.0.130\n",
            "7402\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KdSIuAH2fcGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d95f5e0-f4c0-46ff-b31a-a2d3e99623f6"
      },
      "cell_type": "code",
      "source": [
        "# 判断是否有cuda支持\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Da2EE483fsg1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 设置为 cuDNN benchmark 模式\n",
        "\n",
        "# Benchmark 模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# 如果想要避免这种结果波动，设置\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Note:\n",
        "# 总的来说，大部分情况下，设置这个 benchmark 可以让内置的 cuDNN 的 auto-tuner 自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。\n",
        "# 一般来讲，应该遵循以下准则：\n",
        "# (1)如果网络的输入数据维度或类型上变化不大，设置  torch.backends.cudnn.benchmark = true  可以增加运行效率；\n",
        "# (2)如果网络的输入数据在每次 iteration 都变化的话，会导致 cnDNN 每次都会去寻找一遍最优配置，这样反而会降低运行效率。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Hy2HkZZfhah",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 清除 GPU 存储\n",
        "\n",
        "# 有时 Control-C 中止运行后 GPU 存储没有及时释放，需要手动清空。在 PyTorch 内部可以\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 或在命令行可以先使用 ps 找到程序的 PID，再使用 kill 结束该进程\n",
        "ps aux | grep pythonkill -9 [pid]\n",
        "\n",
        "# 或者直接重置没有被清空的 GPU\n",
        "nvidia-smi --gpu-reset -i [gpu_id]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-MhivXle-Oc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 在训练开始时，参数的初始化是随机的，为了让每次的结果一致，我们需要设置随机种子。\n",
        "\n",
        "# 固定GPU随机化种子\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# 为CPU设置随机种子\n",
        "torch.manual_seed(args.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VFhKbASJg3wE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tensor--------------------------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uiqSKstOhw3n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ac59ba6c-c5ea-4b4a-82d4-53981d78e82f"
      },
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([[1,2,3],[4,5,6]])\n",
        "print(a.type())\n",
        "print(a.size())\n",
        "print(a.dim())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.FloatTensor\n",
            "torch.Size([2, 3])\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jBkEzHtgmO8V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set default tensor type. Float in PyTorch is much faster than double.\n",
        "torch.set_default_tensor_type(torch.FloatTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NCPX7IUwmOAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Type convertions.\n",
        "a = a.cuda() # convert Tensor a on CPU to Tensor a on GPU\n",
        "a = a.cpu() # convert Tensor a on GPU to Tensor a on CPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15tfdr6lnyV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d6eb5d96-d9ba-4371-e834-33146515443a"
      },
      "cell_type": "code",
      "source": [
        "b = a.float() # convert a to FloatTensor\n",
        "print(b.type())\n",
        "c = a.long() # convert a to LongTensor\n",
        "print(c.type())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.FloatTensor\n",
            "torch.LongTensor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ztJLcwAXoHwC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# torch.Tensor -> np.ndarray.\n",
        "a = a.cuda()\n",
        "a = a.cpu().numpy()  # convert to numpy on CPU\n",
        "\n",
        "# np.ndarray -> torch.Tensor.\n",
        "a1 = np.array([[1,2,3],[4,5,6]], dtype=np.int64)\n",
        "b1 = torch.from_numpy(ndarray).float()\n",
        "c1 = torch.from_numpy(ndarray.copy()).float()  # If ndarray has negative stride"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "567skuYBo33M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}